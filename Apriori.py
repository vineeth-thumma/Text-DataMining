from bs4 import BeautifulSoup
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer
from nltk.corpus import stopwords
from collections import OrderedDict
from subprocess import check_call
import random
import time
import csv

stmr = SnowballStemmer('english', ignore_stopwords=True)
tokenizer = RegexpTokenizer(r'[a-zA-Z]+')
stops = set(stopwords.words('english') + ['d'] + ['reuter'] + ['year'] + ['said'])

classwords = []
totaldocs = []
antecedent = []
c = 0

#It will parse the reuters documents and will create two list of lists. Each list in totaldocs refers to a document and has all the words of the document and its classes are of the format "c_class".
#It is done so that we will be able to differentiate between words in class and words in body. The second list antecedent contains only the classes of the document.
def totalWordCreator(xml_filename,writer2):
    xml_file = open(xml_filename, 'r')
    soup = BeautifulSoup(xml_file, 'html.parser')
    bodyTag = soup.findAll('reuters')
    global classwords
    global c
    global totaldocs

    for pTag in bodyTag:
        if pTag.body and pTag.body.getText() and pTag.topics and pTag.topics.getText():
            words = []
            listwords = []
            for i in tokenizer.tokenize(pTag.body.getText()):
                if (i.lower() not in stops) and len(i) > 2 and i not in words:
                    words.append(str(stmr.stem(i)))

            for k in tokenizer.tokenize(str(pTag.topics.findChildren())):
                if (k.lower() not in stops) and len(k) > 2 and k not in listwords:
                    h = "c_"+str(stmr.stem(k))
                    listwords.append(h)
                    if h not in classwords:
                        classwords.append(h)
                        writer2.writerow([h]+["consequent"])

            words = words + listwords
            antecedent.append(listwords)
            totaldocs.append(words)
            c += 1
    xml_file.close()
   
start_time = time.time()
print "Creating Body Word Vector"
i = 100
classlists = open('classlists.csv','w+')
writer1 = csv.writer(classlists)
writer1.writerow(["antecedent"])
while i < 122:
    xml_filename = 'reut2-0'+str(i)[1:]+'.sgm'
    i += 1
    totalWordCreator(xml_filename,writer1)
classlists.close()

#Shuffling both totaldocs and antecedent in the exact same way so that order is not changed.
x = list(zip(antecedent,totaldocs))
random.shuffle(x)
antecedent,totaldocs = zip(*x)
print antecedent[1], totaldocs[1]

#Taking 80% of the documents for training the predictor. Change the 0.8 to other values change the amount data taken to train the predictor.
h = int(0.8*c)
bodylists = open('bodylist.csv','w+')
writer2 = csv.writer(bodylists)

for i in range(h):
    writer2.writerow(totaldocs[i])
bodylists.close()

#Calling the Borglat Apriori program on the bodylist.csv which will contain the entire body and class words. Classlist,csv is appearence file which contains the classes followed by consequent tag.
#So that classes are not used as the antecedents of the rules. To change the values of support or confidence change values corresponding to s or c.
check_call(["/home/9/mandadapu.1/Apriori/apriori/src/apriori","-tr","-s10","-c10","-Rclasslists.csv","bodylist.csv","result.txt"])
w = open('result.txt','r')
sentence = []

#processing the results generated by the apriori program. They will be of the form class <- antecedensts (support, confidence). Change each line into a list of the form [confidence, support, antecedent, consequent].
for line in w:
    part = line.split('<-')
    secondpart = part[1].strip().split(' ')
    secondpart[len(secondpart) - 2] = float(secondpart[len(secondpart) - 2][1:len(secondpart[len(secondpart) - 2])-1])
    secondpart[len(secondpart) - 1] = float(secondpart[len(secondpart) - 1][:len(secondpart[len(secondpart) - 1])-1])
    secondpart.reverse()
    sentence.append(secondpart+[part[0].strip()])

#Sorting te rules by confidence, if confidence is same then by support.
sentence.sort(key=lambda x: (x[0],x[1]),reverse=True)
offline = time.time() - start_time
print ("Offline Cost:", offline)

#The 'd' corresponds to number of rules with highest confidence to be considered at line 110.
predict = OrderedDict()
for i in range(h,c):
    l = set(totaldocs[i])
    predict[i] = []
    d = 0
    for k in sentence:
        if set(k[2:len(k)-1]).issubset(l):
            if k[-1] not in predict[i]:
                predict[i].append(k[-1])
                d += 1
            if d == 3:
                break

#Calculating the accuracy of the classifier. A document is said to correctly classified, if it has even 1 class same.
s = 0
for i in predict.keys():
    if set(predict[i]).intersection(set(antecedent[i])):
        s += 1

print 'accuracy :', s/float(c-h)
total = time.time() - start_time
print "Online Cost:", total - offline
print ("Total time taken:", total)

